# Create user for Hadoop
sudo adduser hadoop

# Switch to the new user
su - hadoop

# Generate SSH key pair
ssh-keygen -t rsa

# Copy the key to authorized key file
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 640 ~/.ssh/authorized_keys

# Test SSH connection to localhost
ssh localhost

## INSTALLING HADOOP
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
tar xzf hadoop-3.3.6.tar.gz
mv hadoop-3.3.6 /usr/local/hadoop

# Add Hadoop environment variables to ~/.bashrc
echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> ~/.bashrc
echo "export HADOOP_HOME=/usr/local/hadoop" >> ~/.bashrc
echo "export HADOOP_INSTALL=\$HADOOP_HOME" >> ~/.bashrc
echo "export HADOOP_MAPRED_HOME=\$HADOOP_HOME" >> ~/.bashrc
echo "export HADOOP_COMMON_HOME=\$HADOOP_HOME" >> ~/.bashrc
echo "export HADOOP_HDFS_HOME=\$HADOOP_HOME" >> ~/.bashrc
echo "export HADOOP_YARN_HOME=\$HADOOP_HOME" >> ~/.bashrc
echo "export HADOOP_COMMON_LIB_NATIVE_DIR=\$HADOOP_HOME/lib/native" >> ~/.bashrc
echo "export PATH=\$PATH:\$HADOOP_HOME/sbin:\$HADOOP_HOME/bin" >> ~/.bashrc
echo "export HADOOP_OPTS=\"-Djava.library.path=\$HADOOP_HOME/lib/native\"" >> ~/.bashrc

# Source the modified ~/.bashrc
source ~/.bashrc

# Configure JAVA_HOME in hadoop-env.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh

# Configure Hadoop
mkdir -p ~/hadoopdata/hdfs/{namenode,datanode}

# Edit core-site.xml
echo "<configuration>" >> $HADOOP_HOME/etc/hadoop/core-site.xml
echo "    <property>" >> $HADOOP_HOME/etc/hadoop/core-site.xml
echo "        <name>fs.defaultFS</name>" >> $HADOOP_HOME/etc/hadoop/core-site.xml
echo "        <value>hdfs://localhost:9000</value>" >> $HADOOP_HOME/etc/hadoop/core-site.xml
echo "    </property>" >> $HADOOP_HOME/etc/hadoop/core-site.xml
echo "</configuration>" >> $HADOOP_HOME/etc/hadoop/core-site.xml

# Edit hdfs-site.xml
echo "<configuration>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "    <property>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "        <name>dfs.replication</name>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "        <value>1</value>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "    </property>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "    <property>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "        <name>dfs.name.dir</name>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "        <value>file:///home/hadoop/hadoopdata/hdfs/namenode</value>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "    </property>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "    <property>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "        <name>dfs.data.dir</name>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "        <value>file:///home/hadoop/hadoopdata/hdfs/datanode</value>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "    </property>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml
echo "</configuration>" >> $HADOOP_HOME/etc/hadoop/hdfs-site.xml

# Edit mapred-site.xml
echo "<configuration>" >> $HADOOP_HOME/etc/hadoop/mapred-site.xml
echo "    <property>" >> $HADOOP_HOME/etc/hadoop/mapred-site.xml
echo "        <name>mapreduce.framework.name</name>" >> $HADOOP_HOME/etc/hadoop/mapred-site.xml
echo "        <value>yarn</value>" >> $HADOOP_HOME/etc/hadoop/mapred-site.xml
echo "    </property>" >> $HADOOP_HOME/etc/hadoop/mapred-site.xml
echo "</configuration>" >> $HADOOP_HOME/etc/hadoop/mapred-site.xml

# Edit yarn-site.xml
echo "<configuration>" >> $HADOOP_HOME/etc/hadoop/yarn-site.xml
echo "    <property>" >> $HADOOP_HOME/etc/hadoop/yarn-site.xml
echo "        <name>yarn.nodemanager.aux-services</name>" >> $HADOOP_HOME/etc/hadoop/yarn-site.xml
echo "        <value>mapreduce_shuffle</value>" >> $HADOOP_HOME/etc/hadoop/yarn-site.xml
echo "    </property>" >> $HADOOP_HOME/etc/hadoop/yarn-site.xml
echo "</configuration>" >> $HADOOP_HOME/etc/hadoop/yarn-site.xml

# Start Hadoop
hdfs namenode -format
start-all.sh

# Check the status of all Hadoop services using jps
jps
